{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> <b> Data Collecting </b></p>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "    <h3 style=\"text-align: left; background-color: #EDC0C7; font-family:newtimeroman; color: black; padding: 14px; line-height: 1; border-radius:10px\"><b>Table of Contents ‚úçÔ∏è</b></h3>\n",
    "    \n",
    "- [1. Introduction](#introduction)\n",
    "    - [1.1 Requirements](#requirements)\n",
    "    - [1.2 Data Sources](#data_sources)\n",
    "    - [1.3 Objectives](#objectives)\n",
    "    - [1.4 Methodology](#methodology)\n",
    "- [2. Implementation](#implementation)\n",
    "    - [Import libraries](#import-libraries)\n",
    "    - [Collect urls of the top 10000 manga](#collect_urls)\n",
    "    - [Collect data of each manga](#collect_data)\n",
    "    - [Save the data](#save_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## <div style=\"text-align: left; background-color:#EDC0C7; font-family:newtimeroman;color: black; padding: 14px; line-height: 1;border-radius:10px\">1. Introduction</div>\n",
    "\n",
    "\n",
    "<a class=\"anchor\" id=\"requirements\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> 1.1 Requirements  </span>\n",
    "- Our group will be responsible for collecting and preparing the data for analysis and exploration.\n",
    "- The data will be collected from the Internet by parsing HTML code or using APIs. Our group are not allowed to use available datasets.\n",
    "- Our dataset must have at least 5 fields and 1000 observations.\n",
    "\n",
    "<a class=\"anchor\" id=\"data_sources\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> 1.2 Data Sources  </span>\n",
    "- **Topic**: Manga (Japanese comics)\n",
    "- **Website**: [MyAnimeList](https://myanimelist.net/)\n",
    "\n",
    "<a class=\"anchor\" id=\"objectives\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> 1.3 Objectives  </span>\n",
    "- Crawl data of the top 10000 manga on MyAnimeList.\n",
    "\n",
    "- Data attributes:\n",
    "    - Title: title of the manga.\n",
    "    - Score: average score of the manga.\n",
    "    - Vote: number of votes for the manga.\n",
    "    - Ranked: rank of the manga.\n",
    "    - Popularity: popularity of the manga.\n",
    "    - Members: number of members who have added this manga to their list.\n",
    "    - Favorite: number of members who have this manga in their list.\n",
    "    - Volumes: number of volumes of the manga.\n",
    "    - Chapters: number of chapters of the manga.\n",
    "    - Status: status of the manga (Publishing, Finished).\n",
    "    - Published: date of publication of the manga.\n",
    "    - Genres: genres of the manga.\n",
    "    - Themes: themes of the manga.\n",
    "    - Author: author of the manga.\n",
    "    - Total Review: total reviews of the manga.\n",
    "    - Type Review: type of reviews of the manga.\n",
    "    \n",
    "- Save the data in a CSV file.\n",
    "\n",
    "<a class=\"anchor\" id=\"methodology\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> 1.4 Methodology  </span>\n",
    "1. Start at the [Top Manga page](https://myanimelist.net/topmanga.php?limit=0) and crawl the urls of the top 10000 manga by changing the limit parameter.\n",
    "\n",
    "2. Iterate through the urls and crawl the html code of each manga.\n",
    "\n",
    "3. Parse the html code of each manga to get the data.\n",
    "\n",
    "4. Save the data in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"implementation\"></a>\n",
    "## <div style=\"text-align: left; background-color:#EDC0C7; font-family:newtimeroman;color: black; padding: 14px; line-height: 1;border-radius:10px\">2. Implementation</div>\n",
    "\n",
    "<a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> üìï Import libraries </span>\n",
    "üëâ The following libraries are used in this notebook:\n",
    "- `requests`: Used to make HTTP requests to the website and get the HTML content\n",
    "\n",
    "- `HTMLSession`: Used to get the HTML content of the dynamic website (the website uses JavaScript to render the content).  \n",
    "\n",
    "- `BeautifulSoup`: Used for parsing HTML code, extracting data from HTML code.\n",
    "\n",
    "- `re`: Used to pattern matching and extracting data from HTML content using regular expressions.\n",
    "\n",
    "- `nest_asyncio`: enables the use of asynchronous code in Jupyter notebooks, addressing compatibility issues with asyncio.\n",
    "\n",
    "- `pandas`: Used to organize and manipulate the extracted data, creating a structured DataFrame,, which are tabular data structures.\n",
    "\n",
    "- `datetime`: Used to timestamp the data collection process, providing information about when the data was collected.\n",
    "\n",
    "- `time`: Used to pause the execution of the code for a specified amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nest_asyncio\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"collect_urls\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> üìï Collect urls of the top 10000 manga </span>\n",
    "üëâ Steps to collect the urls:\n",
    "\n",
    "1. Send a GET request to the [Top Manga page](https://myanimelist.net/topmanga.php?limit=0) to get the HTML content of the page.\n",
    "\n",
    "2. Parse the HTML content using `BeautifulSoup`.\n",
    "\n",
    "3. Find and extract the urls from the HTML content.\n",
    "\n",
    "4. Save the urls in a list.\n",
    "\n",
    "5. Repeat the above steps by increasing the limit parameter by 50 each time until the limit parameter reaches 10000.\n",
    "\n",
    "The collecting process is split into 2 parts, each part collects 5000 urls to avoid the connection being interrupted by the website due to too many requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Applying `nest_asyncio` to avoid issues that may arise from nested event loops when working with asynchronous code.\n",
    "\n",
    "- Creating an HTML Session using the `HTMLSession` class from the `requests_html` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply() \n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Crawl the first 5000 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 urls collected\r"
     ]
    }
   ],
   "source": [
    "listUrl1 = []\n",
    "\n",
    "for i in range(0, 5000, 50):\n",
    "    # Url of the website to scrap\n",
    "    url = f'https://myanimelist.net/topmanga.php?limit={i}'\n",
    "\n",
    "    # Get the html content\n",
    "    html = requests.get(url).text\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Get the list of manga\n",
    "    listItem = soup.find_all(\"td\", {\"class\": \"title al va-t clearfix word-break\"})\n",
    "\n",
    "    # Get the url of each manga\n",
    "    for item in listItem:\n",
    "        listUrl1.append(item.find('a').get('href'))\n",
    "\n",
    "    # Print the number of manga urls collected\n",
    "    print(f'{len(listUrl1)} urls collected', end='\\r', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Crawl the remaining 5000 urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to data collection above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 urls collected\r"
     ]
    }
   ],
   "source": [
    "listUrl2 = []\n",
    "\n",
    "for i in range(5000,10000,50):\n",
    "    # Url of the website to scrap\n",
    "    url = f'https://myanimelist.net/topmanga.php?limit={i}'\n",
    "\n",
    "    # Get the html content\n",
    "    html = requests.get(url).text\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Get the list of manga\n",
    "    listItem = soup.find_all(\"td\", {\"class\": \"title al va-t clearfix word-break\"})\n",
    "\n",
    "    # Get the url of each manga\n",
    "    for item in listItem:\n",
    "        listUrl2.append(item.find('a').get('href'))\n",
    "\n",
    "    # Print the number of manga urls collected\n",
    "    print(f'{len(listUrl2)} urls collected', end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Concatenate 2 list urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10000 urls collected\n"
     ]
    }
   ],
   "source": [
    "listUrl = listUrl1 + listUrl2\n",
    "print(f'Total: {len(listUrl)} urls collected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"collect_data\"></a>\n",
    "\n",
    "## <span style='color:#2B9C15 '> üìï Collect data of each manga  </span>\n",
    "1. From each url collected above, send a GET request to get the HTML content of the page.\n",
    "2. If length of the HTML content is smaller than 4000 , sleep for 10 seconds and send the GET request again. Because that means the website has blocked the connection and we need to wait for a while before sending the request again.\n",
    "3. Save the HTML content in a list for parsing later.\n",
    "\n",
    "This process still splits into 2 parts, each part collects 5000 HTML contents to avoid the connection being interrupted by the website due to too many requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Crawl HTML content from the first 5000 manga URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/10000 manga html collected\r"
     ]
    }
   ],
   "source": [
    "listHtml1 = []\n",
    "\n",
    "for url in listUrl[:5000]:\n",
    "    res = session.get(url)\n",
    "    while len(res.text) < 4000:\n",
    "        # Sleep for 10 minutes\n",
    "        time.sleep(600)\n",
    "        res = session.get(url)\n",
    "        \n",
    "    listHtml1.append(res.text)\n",
    "\n",
    "    # Print the number of manga html collected\n",
    "    print(f'{len(listHtml1)}/{len(listUrl)} manga html collected', end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Crawl HTML content from the remaining 5000 manga URLs\n",
    "- Similar to data collection above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 manga html collected\r"
     ]
    }
   ],
   "source": [
    "listHtml2 = []\n",
    "\n",
    "for url in listUrl[5000:]:\n",
    "    res = session.get(url)\n",
    "    while len(res.text) < 4000:\n",
    "        # Sleep for 10 minutes\n",
    "        time.sleep(600)\n",
    "        res = session.get(url)\n",
    "        \n",
    "    listHtml2.append(res.text)\n",
    "\n",
    "    # Print the number of manga html collected\n",
    "    print(f'{len(listHtml2)+5000}/{len(listUrl)} manga html collected', end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of data collection:  2023-12-05\n"
     ]
    }
   ],
   "source": [
    "# Extract time of data collection to report for the project\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d\")\n",
    "print(\"Time of data collection: \", now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Concatenate 2 list htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10000 manga html collected\n"
     ]
    }
   ],
   "source": [
    "listHtml = listHtml1 + listHtml2\n",
    "print(f'Total: {len(listHtml)} manga html collected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Extracting the detailed values of each comic website page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parsing HTML Content: The function starts by using BeautifulSoup to parse the HTML content of a comic page\n",
    "\n",
    "2. Extracting Title:\n",
    "    - The title of the comic is extracted using `soup.find('span', {'itemprop': 'name'})`\n",
    "    - If the title is not found (i.e., None), the function returns None to indicate that the information couldn't be extracted\n",
    "\n",
    "3. Handling English Title (title-english):\n",
    "    - If an English title is present (indicated by the presence of a title-english span), it is extracted and removed from the main title. The resulting title is a combination of the original title and the English title enclosed in parentheses\n",
    "\n",
    "4. Extracting Rating Information:\n",
    "    - The rating information is extracted using `soup.find('span', {'itemprop': 'ratingValue'}).text` and `soup.find('span', {'itemprop': 'ratingCount'}).text`\n",
    "    - These represent the rating value and the count of ratings, respectively\n",
    "\n",
    "5. Extracting Rank and Popularity:\n",
    "    - The rank and popularity are extracted using regular expressions `(re.findall)`\n",
    "    - The regular expression `r'\\d+'` is used to find all sequences of digits in the text, and [0] is used to select the first match\n",
    "\n",
    "6. Looping Through Information Sections:\n",
    "    - The function iterates through the information sections of the manga page, represented by `div` elements with the `class 'spaceit_pad'`\n",
    "\n",
    "7. Extracting Manga Details:\n",
    "    - For each section, it checks the content and extracts relevant details such as `volumes`, `chapters`, `status`, `published date`, `genres`, `themes`, `authors`, `favorites`, and `members`\n",
    "    - The extracted information is stored in the respective variables\n",
    "\n",
    "8. Extracting Review Information:\n",
    "    - The function then moves to the `'manga-info-review__header'` section to extract information related to reviews\n",
    "    - It retrieves the `total number` of reviews and the number of reviews for each type (`recommended`, `mixed-feelings`, `not-recommended`)\n",
    "\n",
    "9. Returning a Dictionary:  \n",
    "    - The function compiles all the extracted information into a dictionary and convert to Pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(htmlComic):\n",
    "    soup = BeautifulSoup(htmlComic, \"html.parser\")\n",
    "\n",
    "    title = soup.find('span', {'itemprop': 'name'})\n",
    "    if title is None:\n",
    "        return None\n",
    "    else:\n",
    "        title_text = title.text.strip()\n",
    "        title_english_span = title.find('span', {'class': 'title-english'})\n",
    "\n",
    "        if title_english_span is not None:\n",
    "            title_english_text = title_english_span.text.strip()\n",
    "            title_text = title_text.replace(title_english_text, '')\n",
    "            title = f'{title_text} ({title_english_text})'\n",
    "        else:\n",
    "            title = title_text\n",
    "    ratingValue = soup.find('span', {'itemprop': 'ratingValue'}).text\n",
    "    ratingCount = soup.find('span', {'itemprop': 'ratingCount'}).text\n",
    "    ranked = re.findall(r'\\d+', soup.find('span', {'class': 'numbers ranked'}).text)[0]\n",
    "    popularity = re.findall(r'\\d+', soup.find('span', {'class': 'numbers popularity'}).text)[0]\n",
    "\n",
    "    volumes, chapters, status, published = '', '', '', ''\n",
    "    genres, themes, authors, favorites, members = [], [], '', '', ''\n",
    "\n",
    "    for space in soup.find_all(\"div\", {'class': 'spaceit_pad'}):\n",
    "        text = space.text\n",
    "        if 'Volumes' in text:\n",
    "            volumes = text.split(':')[1].strip()\n",
    "        elif 'Chapters' in text:\n",
    "            chapters = text.split(':')[1].strip()\n",
    "        elif 'Status' in text:\n",
    "            status = text.split(':')[1].strip()\n",
    "        elif 'Published' in text:\n",
    "            published = text.split(':')[1].strip()\n",
    "        elif 'Genres' in text:\n",
    "            genres = [gen.text for gen in space.find_all('span', {'itemprop': 'genre'})]\n",
    "        elif 'Themes' in text:\n",
    "            themes = [theme.text for theme in space.find_all('span', {'itemprop': 'genre'})]\n",
    "        elif 'Authors' in text:\n",
    "            authors = text.split(':')[1].strip()\n",
    "        elif 'Favorites' in text:\n",
    "            favorites = text.split(':')[1].strip()\n",
    "        elif 'Members' in text:\n",
    "            members = text.split(':')[1].strip()\n",
    "\n",
    "    infoReviews = soup.find('div', {'class': 'manga-info-review__header mal-navbar'})\n",
    "    totalReviews = re.findall(r'\\d+', infoReviews.find('div', {'class': 'right'}).text)[0]\n",
    "\n",
    "    typeReview = [\n",
    "        int(re.findall(r'\\d+', infoReviews.find('div', {'class': 'recommended'}).text)[0]),\n",
    "        int(re.findall(r'\\d+', infoReviews.find('div', {'class': 'mixed-feelings'}).text)[0]),\n",
    "        int(re.findall(r'\\d+', infoReviews.find('div', {'class': 'not-recommended'}).text)[0])\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"Title\": title, \"Score\": ratingValue, \"Vote\": ratingCount,\n",
    "        \"Ranked\": ranked, \"Popularity\": popularity, \"Members\": members,\n",
    "        \"Favorite\": favorites, \"Volumes\": volumes, \"Chapters\": chapters,\n",
    "        \"Status\": status, \"Published\": published, \"Genres\": genres,\n",
    "        \"Themes\": themes, \"Author\": authors, \"Total Review\": totalReviews,\n",
    "        \"Type Review\": typeReview\n",
    "    }\n",
    "\n",
    "data_list = [extract_info(htmlComic) for htmlComic in listHtml if extract_info(htmlComic) is not None]\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëâ Returns the first 5 rows to check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>Vote</th>\n",
       "      <th>Ranked</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Members</th>\n",
       "      <th>Favorite</th>\n",
       "      <th>Volumes</th>\n",
       "      <th>Chapters</th>\n",
       "      <th>Status</th>\n",
       "      <th>Published</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Themes</th>\n",
       "      <th>Author</th>\n",
       "      <th>Total Review</th>\n",
       "      <th>Type Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berserk</td>\n",
       "      <td>9.47</td>\n",
       "      <td>331288</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>665,300</td>\n",
       "      <td>122,841</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Aug  25, 1989 to ?</td>\n",
       "      <td>[Action, Adventure, Award Winning, Drama, Fant...</td>\n",
       "      <td>[Gore, Military, Mythology, Psychological]</td>\n",
       "      <td>Miura, Kentarou (Story &amp; Art), Studio Gaga (Art)</td>\n",
       "      <td>258</td>\n",
       "      <td>[233, 15, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JoJo no Kimyou na Bouken Part 7: Steel Ball Run</td>\n",
       "      <td>9.30</td>\n",
       "      <td>156368</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>256,146</td>\n",
       "      <td>42,864</td>\n",
       "      <td>24</td>\n",
       "      <td>96</td>\n",
       "      <td>Finished</td>\n",
       "      <td>Jan  19, 2004 to Apr  19, 2011</td>\n",
       "      <td>[Action, Adventure, Mystery, Supernatural]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Araki, Hirohiko (Story &amp; Art)</td>\n",
       "      <td>128</td>\n",
       "      <td>[120, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vagabond</td>\n",
       "      <td>9.24</td>\n",
       "      <td>136403</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>364,891</td>\n",
       "      <td>40,158</td>\n",
       "      <td>37</td>\n",
       "      <td>327</td>\n",
       "      <td>On Hiatus</td>\n",
       "      <td>Sep  3, 1998 to May  21, 2015</td>\n",
       "      <td>[Action, Adventure, Award Winning]</td>\n",
       "      <td>[Historical, Samurai]</td>\n",
       "      <td>Inoue, Takehiko (Story &amp; Art), Yoshikawa, Eiji...</td>\n",
       "      <td>97</td>\n",
       "      <td>[88, 8, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One Piece</td>\n",
       "      <td>9.22</td>\n",
       "      <td>366668</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>599,278</td>\n",
       "      <td>114,531</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Jul  22, 1997 to ?</td>\n",
       "      <td>[Action, Adventure, Fantasy]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Oda, Eiichiro (Story &amp; Art)</td>\n",
       "      <td>206</td>\n",
       "      <td>[173, 17, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monster</td>\n",
       "      <td>9.15</td>\n",
       "      <td>93945</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>236,355</td>\n",
       "      <td>20,501</td>\n",
       "      <td>18</td>\n",
       "      <td>162</td>\n",
       "      <td>Finished</td>\n",
       "      <td>Dec  5, 1994 to Dec  20, 2001</td>\n",
       "      <td>[Award Winning, Drama, Mystery]</td>\n",
       "      <td>[Adult Cast, Psychological]</td>\n",
       "      <td>Urasawa, Naoki (Story &amp; Art)</td>\n",
       "      <td>76</td>\n",
       "      <td>[64, 7, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Title Score    Vote Ranked  \\\n",
       "0                                          Berserk  9.47  331288      1   \n",
       "1  JoJo no Kimyou na Bouken Part 7: Steel Ball Run  9.30  156368      2   \n",
       "2                                         Vagabond  9.24  136403      3   \n",
       "3                                        One Piece  9.22  366668      4   \n",
       "4                                          Monster  9.15   93945      5   \n",
       "\n",
       "  Popularity  Members Favorite  Volumes Chapters      Status  \\\n",
       "0          1  665,300  122,841  Unknown  Unknown  Publishing   \n",
       "1         26  256,146   42,864       24       96    Finished   \n",
       "2         15  364,891   40,158       37      327   On Hiatus   \n",
       "3          3  599,278  114,531  Unknown  Unknown  Publishing   \n",
       "4         29  236,355   20,501       18      162    Finished   \n",
       "\n",
       "                        Published  \\\n",
       "0              Aug  25, 1989 to ?   \n",
       "1  Jan  19, 2004 to Apr  19, 2011   \n",
       "2   Sep  3, 1998 to May  21, 2015   \n",
       "3              Jul  22, 1997 to ?   \n",
       "4   Dec  5, 1994 to Dec  20, 2001   \n",
       "\n",
       "                                              Genres  \\\n",
       "0  [Action, Adventure, Award Winning, Drama, Fant...   \n",
       "1         [Action, Adventure, Mystery, Supernatural]   \n",
       "2                 [Action, Adventure, Award Winning]   \n",
       "3                       [Action, Adventure, Fantasy]   \n",
       "4                    [Award Winning, Drama, Mystery]   \n",
       "\n",
       "                                       Themes  \\\n",
       "0  [Gore, Military, Mythology, Psychological]   \n",
       "1                                          []   \n",
       "2                       [Historical, Samurai]   \n",
       "3                                          []   \n",
       "4                 [Adult Cast, Psychological]   \n",
       "\n",
       "                                              Author Total Review  \\\n",
       "0   Miura, Kentarou (Story & Art), Studio Gaga (Art)          258   \n",
       "1                      Araki, Hirohiko (Story & Art)          128   \n",
       "2  Inoue, Takehiko (Story & Art), Yoshikawa, Eiji...           97   \n",
       "3                        Oda, Eiichiro (Story & Art)          206   \n",
       "4                       Urasawa, Naoki (Story & Art)           76   \n",
       "\n",
       "     Type Review  \n",
       "0  [233, 15, 10]  \n",
       "1    [120, 7, 1]  \n",
       "2     [88, 8, 1]  \n",
       "3  [173, 17, 16]  \n",
       "4     [64, 7, 5]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"save_data\"></a>\n",
    "## <span style='color:#2B9C15 '> üìï Save the data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/raw_comic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; background-color:#EDC0C7; font-family:Arial; color:black; padding: 12px; line-height:1.25;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 30px;border-style: solid;border-color: black;\">END</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
